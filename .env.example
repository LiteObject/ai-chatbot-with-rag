# =============================================================================
# API Keys (set the one for your chosen provider)
# =============================================================================

# OpenAI - Get yours at https://platform.openai.com/api-keys
OPENAI_API_KEY=your-api-key-here

# Azure OpenAI (uncomment if using Azure)
# AZURE_OPENAI_API_KEY=your-azure-key-here
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
# AZURE_OPENAI_API_VERSION=2024-02-01
# AZURE_DEPLOYMENT_NAME=your-deployment-name
# AZURE_EMBEDDING_DEPLOYMENT=your-embedding-deployment

# Anthropic (uncomment if using Claude)
# ANTHROPIC_API_KEY=your-anthropic-key-here

# =============================================================================
# LLM Provider Configuration
# =============================================================================
# Options: openai, azure, anthropic, ollama
LLM_PROVIDER=openai

# Model name (varies by provider)
# OpenAI: gpt-4o-mini, gpt-4o, gpt-4-turbo
# Azure: your deployment name
# Anthropic: claude-3-5-sonnet-20241022, claude-3-opus-20240229
# Ollama: llama3.2, mistral, codellama
CHAT_MODEL=gpt-4o-mini
CHAT_TEMPERATURE=0.7

# =============================================================================
# Embedding Provider Configuration
# =============================================================================
# Options: openai, azure, huggingface, ollama
EMBEDDING_PROVIDER=openai

# Embedding model name (varies by provider)
# OpenAI: text-embedding-3-small, text-embedding-3-large
# Azure: your embedding deployment name
# HuggingFace: sentence-transformers/all-MiniLM-L6-v2
# Ollama: nomic-embed-text, mxbai-embed-large
EMBEDDING_MODEL=text-embedding-3-small

# =============================================================================
# Vector Store Configuration
# =============================================================================
# Options: chroma, faiss
VECTOR_STORE_TYPE=chroma

# =============================================================================
# Ollama Configuration (if using local models)
# =============================================================================
# OLLAMA_BASE_URL=http://localhost:11434
